{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f96d9b",
   "metadata": {},
   "source": [
    "# Building X-Ray Classification Convolutional Neural Network Model\n",
    "\n",
    "<img src=\"https://static.dw.com/image/16373135_401.jpg\" alt=\"dw\">\n",
    "\n",
    "## Content\n",
    "\n",
    "The dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (Pneumonia/Normal). There are 5,863 X-Ray images (JPEG) and 2 categories (Pneumonia/Normal).\n",
    "\n",
    "Chest X-ray images (anterior-posterior) were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children’s Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patients’ routine clinical care.\n",
    "\n",
    "For the analysis of chest x-ray images, all chest radiographs were initially screened for quality control by removing all low quality or unreadable scans. The diagnoses for the images were then graded by two expert physicians before being cleared for training the AI system. In order to account for any grading errors, the evaluation set was also checked by a third expert.\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "Data: https://data.mendeley.com/datasets/rscbjbr9sj/2\n",
    "\n",
    "License: CC BY 4.0\n",
    "\n",
    "Citation: http://www.cell.com/cell/fulltext/S0092-8674(18)30154-5\n",
    "\n",
    "## Dataset and Sources\n",
    "\n",
    "Picture: https://static.dw.com/image/16373135_401.jpg\n",
    "\n",
    "Dataset: https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22852c50",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60788599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "080c9b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier = load_model('model.h5')\n",
    "#classifier.summary()\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e11f6de",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2433bfed",
   "metadata": {},
   "source": [
    "### Import image data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dff4fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417ae001",
   "metadata": {},
   "source": [
    "ImadeDataGenerator library for pictures. The difference from normal picture readings is that it evaluates the pictures one by one, not all at once and helps the RAM to work in a healthy way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32ff898f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## shear_range = Side bends\n",
    "## zoom_range = Zoom\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "## target_size= 64x64 size pictures for scan.\n",
    "## class_mode= Binary set\n",
    "training_set = train_datagen.flow_from_directory(\"chest_xray/train\",\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory(\"chest_xray/test\",\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bc338b",
   "metadata": {},
   "source": [
    "## Building Convolutional Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e3d76ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62693cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))\n",
    "classifier.add(MaxPool2D(pool_size=2, strides=2))\n",
    "classifier.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "classifier.add(MaxPool2D(pool_size=2, strides=2))\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(units=128, activation='relu'))\n",
    "classifier.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc7beb",
   "metadata": {},
   "source": [
    "### Train Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6bdf30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5ff0ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "163/163 [==============================] - 67s 409ms/step - loss: 0.3851 - accuracy: 0.8342 - val_loss: 0.2885 - val_accuracy: 0.8798\n",
      "Epoch 2/25\n",
      "163/163 [==============================] - 60s 369ms/step - loss: 0.2219 - accuracy: 0.9070 - val_loss: 0.3036 - val_accuracy: 0.8718\n",
      "Epoch 3/25\n",
      "163/163 [==============================] - 59s 363ms/step - loss: 0.2248 - accuracy: 0.9053 - val_loss: 0.2881 - val_accuracy: 0.8734\n",
      "Epoch 4/25\n",
      "163/163 [==============================] - 58s 355ms/step - loss: 0.2061 - accuracy: 0.9189 - val_loss: 0.2661 - val_accuracy: 0.9006\n",
      "Epoch 5/25\n",
      "163/163 [==============================] - 60s 369ms/step - loss: 0.1851 - accuracy: 0.9241 - val_loss: 0.4840 - val_accuracy: 0.8301\n",
      "Epoch 6/25\n",
      "163/163 [==============================] - 62s 383ms/step - loss: 0.1727 - accuracy: 0.9304 - val_loss: 0.4411 - val_accuracy: 0.8462\n",
      "Epoch 7/25\n",
      "163/163 [==============================] - 62s 381ms/step - loss: 0.1630 - accuracy: 0.9344 - val_loss: 0.3260 - val_accuracy: 0.8782\n",
      "Epoch 8/25\n",
      "163/163 [==============================] - 60s 370ms/step - loss: 0.1529 - accuracy: 0.9408 - val_loss: 0.3249 - val_accuracy: 0.8750\n",
      "Epoch 9/25\n",
      "163/163 [==============================] - 63s 389ms/step - loss: 0.1468 - accuracy: 0.9411 - val_loss: 0.2702 - val_accuracy: 0.9071\n",
      "Epoch 10/25\n",
      "163/163 [==============================] - 67s 408ms/step - loss: 0.1384 - accuracy: 0.9492 - val_loss: 0.2680 - val_accuracy: 0.9006\n",
      "Epoch 11/25\n",
      "163/163 [==============================] - 64s 391ms/step - loss: 0.1500 - accuracy: 0.9450 - val_loss: 0.3857 - val_accuracy: 0.8718\n",
      "Epoch 12/25\n",
      "163/163 [==============================] - 63s 386ms/step - loss: 0.1418 - accuracy: 0.9457 - val_loss: 0.2942 - val_accuracy: 0.9038\n",
      "Epoch 13/25\n",
      "163/163 [==============================] - 64s 392ms/step - loss: 0.1281 - accuracy: 0.9517 - val_loss: 0.2445 - val_accuracy: 0.9087\n",
      "Epoch 14/25\n",
      "163/163 [==============================] - 59s 363ms/step - loss: 0.1297 - accuracy: 0.9538 - val_loss: 0.4553 - val_accuracy: 0.8478\n",
      "Epoch 15/25\n",
      "163/163 [==============================] - 63s 387ms/step - loss: 0.1250 - accuracy: 0.9492 - val_loss: 0.2378 - val_accuracy: 0.9119\n",
      "Epoch 16/25\n",
      "163/163 [==============================] - 62s 378ms/step - loss: 0.1224 - accuracy: 0.9544 - val_loss: 0.2282 - val_accuracy: 0.9231\n",
      "Epoch 17/25\n",
      "163/163 [==============================] - 61s 376ms/step - loss: 0.1168 - accuracy: 0.9563 - val_loss: 0.3147 - val_accuracy: 0.8942\n",
      "Epoch 18/25\n",
      "163/163 [==============================] - 62s 383ms/step - loss: 0.1177 - accuracy: 0.9559 - val_loss: 0.3257 - val_accuracy: 0.9054\n",
      "Epoch 19/25\n",
      "163/163 [==============================] - 58s 357ms/step - loss: 0.1120 - accuracy: 0.9565 - val_loss: 0.4265 - val_accuracy: 0.8734\n",
      "Epoch 20/25\n",
      "163/163 [==============================] - 62s 381ms/step - loss: 0.1187 - accuracy: 0.9540 - val_loss: 0.3941 - val_accuracy: 0.8590\n",
      "Epoch 21/25\n",
      "163/163 [==============================] - 63s 390ms/step - loss: 0.1058 - accuracy: 0.9618 - val_loss: 0.3210 - val_accuracy: 0.9054\n",
      "Epoch 22/25\n",
      "163/163 [==============================] - 63s 388ms/step - loss: 0.1129 - accuracy: 0.9557 - val_loss: 0.3299 - val_accuracy: 0.8942\n",
      "Epoch 23/25\n",
      "163/163 [==============================] - 60s 369ms/step - loss: 0.0953 - accuracy: 0.9632 - val_loss: 0.5210 - val_accuracy: 0.8558\n",
      "Epoch 24/25\n",
      "163/163 [==============================] - 69s 422ms/step - loss: 0.1018 - accuracy: 0.9649 - val_loss: 0.3769 - val_accuracy: 0.8862\n",
      "Epoch 25/25\n",
      "163/163 [==============================] - 61s 371ms/step - loss: 0.0956 - accuracy: 0.9653 - val_loss: 0.3393 - val_accuracy: 0.9054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f64f457d30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x = training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cccea59",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3da56a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               802944    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 813,217\n",
      "Trainable params: 813,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.save('model.h5')\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd53392",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b077807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 5s 235ms/step\n",
      "Prediction successful.\n"
     ]
    }
   ],
   "source": [
    "test_set.reset()\n",
    "pred=classifier.predict_generator(test_set, verbose=1)\n",
    "\n",
    "## Filter predictions\n",
    "pred[pred > .5] = 1\n",
    "pred[pred <= .5] = 0\n",
    "print('Prediction successful.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33af5b3d",
   "metadata": {},
   "source": [
    "### Creating confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d0ff3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "147fb81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Labels(test_labels):\n",
      "\n",
      "[1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "test_labels = []\n",
    "for i in range(0, int(20)):\n",
    "    test_labels.extend(np.array(test_set[i][1]))\n",
    "print('Test Labels(test_labels):\\n')\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24845612",
   "metadata": {},
   "source": [
    "How each file was estimated and compared with real data is shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2eb8124",
   "metadata": {},
   "outputs": [],
   "source": [
    "dosyaisimleri = test_set.filenames   \n",
    "sonuc = pd.DataFrame()\n",
    "sonuc['dosyaisimleri'] = dosyaisimleri\n",
    "sonuc['tahminler'] = pred\n",
    "sonuc['test'] = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93ae729a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[ 74 160]\n",
      " [123 267]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(test_labels, pred)\n",
    "print (\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4450a08",
   "metadata": {},
   "source": [
    "### Prediction from Valitadion Pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08343a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d0b2c7",
   "metadata": {},
   "source": [
    "#### Predict NORMAL class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cd28fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction of NORMAL class\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[0.]]\n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "image = image\n",
    "image_name=[1427, 1430, 1431, 1436, 1437, 1438, 1440, 1442]\n",
    "print('\\nPrediction of NORMAL class')\n",
    "for i in image_name:\n",
    "    i = str(i)\n",
    "    path = 'chest_xray/val/NORMAL/NORMAL2-IM-' + i + '-0001' + '.jpeg'\n",
    "    img = image.load_img(path, target_size=(64, 64))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0) \n",
    "    images = np.vstack([x])\n",
    "    classes_normal = classifier.predict(images, batch_size=1)\n",
    "    print(classes_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f93ac96",
   "metadata": {},
   "source": [
    "#### Predict PNEUMONIA class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d397c755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction of PNEUMONIA class\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n",
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "person=[1946, 1947, 1949, 1950, 1951, 1952, 1954]\n",
    "bacteria=[4875, 4876, 4880, 4881, 4882, 4883, 4886]\n",
    "print('\\nPrediction of PNEUMONIA class')\n",
    "for i in person:\n",
    "    index=int(person.index(i))\n",
    "    bac=bacteria[index]  \n",
    "    \n",
    "    bac = str(bac)\n",
    "    i = str(i)    \n",
    "    path = 'chest_xray/val/PNEUMONIA/person' + i + '_bacteria_'+ bac +'.jpeg'\n",
    "    img = image.load_img(path, target_size=(64, 64))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0) \n",
    "    images = np.vstack([x])\n",
    "    classes_pneumonia = classifier.predict(images, batch_size=1)\n",
    "    print(classes_pneumonia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e6ab1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Took 1857.7696058750153 seconds to classificate objects.\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "cal_time = end - start\n",
    "print(\"\\nTook {} seconds to classificate objects.\".format(cal_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f50e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
